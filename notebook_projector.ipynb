{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8912b7d-7a53-4643-8e00-66fb8ba8c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "device = \"cuda\"\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "from nokogiri.working_dir import working_dir\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "import projector\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from script_util import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update(rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = Path(\"/data/natsuki/training116/00030-v4-mirror-auto4-gamma100-batch64-noaug-resumecustom/network-snapshot-011289.pkl\")\n",
    "assert fm.is_file()\n",
    "with open(fm, 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema'].to(device)\n",
    "def mapping(seed=1, psi=1):\n",
    "    label = torch.zeros([1, G.c_dim], device=device)\n",
    "    z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
    "    w = G.mapping(z, label, truncation_psi=psi)\n",
    "    return w\n",
    "def synthesis(w, synth=True):\n",
    "    if synth:\n",
    "        synth_image = G.synthesis(w, noise_mode='const')\n",
    "    else:\n",
    "        synth_image = w\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    img = PIL.Image.fromarray(synth_image, 'RGB')\n",
    "    return img\n",
    "def load_target(target_fname):\n",
    "    target_pil = PIL.Image.open(target_fname).convert('RGB')\n",
    "    w, h = target_pil.size\n",
    "    s = min(w, h)\n",
    "    target_pil_resized = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "    target_pil_resized = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
    "    target_uint8 = np.array(target_pil_resized, dtype=np.uint8)\n",
    "    target_tensor = torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)\n",
    "    return target_tensor, target_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dfbef-bcc2-4599-8cc7-d593942e966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with working_dir(\"/home/natsuki/bizarre-pose-estimator\"):\n",
    "    from _train.danbooru_tagger.models.kate import Model as DanbooruTagger\n",
    "    danbooru_tagger = DanbooruTagger.load_from_checkpoint(\n",
    "        './_train/danbooru_tagger/runs/waning_kate_vulcan0001/checkpoints/'\n",
    "        'epoch=0022-val_f2=0.4461-val_loss=0.0766.ckpt'\n",
    "    )\n",
    "    danbooru_tagger.eval()\n",
    "    danbooru_tagger.cuda()\n",
    "    for param in danbooru_tagger.parameters():\n",
    "        param.requires_grad = False\n",
    "def biz_feat(imgs):\n",
    "    imgs = F.interpolate(imgs, size=(256, 256), mode='area')\n",
    "    imgs = (imgs-imgs.min())/(imgs.max()-imgs.min())\n",
    "    feat = danbooru_tagger(imgs)[\"raw\"]\n",
    "    return feat\n",
    "def biz_loss(target, synth):\n",
    "    dist = danbooru_tagger.loss(torch.sigmoid(target), synth)[\"loss\"]\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(projector)\n",
    "target_tensor, target_pil = load_target(list(Path(\"/data/natsuki/danbooru2020/v4/0000/\").glob(\"*.png\"))[0])\n",
    "projected_w_steps = projector.project(\n",
    "    G,\n",
    "    target=target_tensor,\n",
    "    num_steps=100,\n",
    "    device=device,\n",
    "    yield_more=True,\n",
    "    additional_feat=biz_feat,\n",
    "    additional_loss=biz_loss,\n",
    "    additional_weight=1,\n",
    ")\n",
    "display(target_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56785ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = defaultdict(list)\n",
    "for data in projected_w_steps:\n",
    "    for k, v in data.items():\n",
    "        record[k].append(v)\n",
    "    clear_output(wait=True)\n",
    "    display(data[\"pil\"], data[\"dist\"], data[\"additional_dist\"], data[\"log\"])\n",
    "#    fig = plt.figure()\n",
    "#    fig, axs = plt.subplots(1, 3)\n",
    "#    axs[0].plot(record[\"loss\"])\n",
    "#    axs[1].imshow(target_pil)\n",
    "#    axs[2].imshow(data[\"pil\"])\n",
    "#    display(plt.gcf())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93908de20a6c520c67f7bfb8c544721ea1883c17bbecd61a6d61de1167f09900"
  },
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
